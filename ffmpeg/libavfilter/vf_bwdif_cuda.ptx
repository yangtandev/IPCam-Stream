//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32965470
// Cuda compilation tools, release 12.2, V12.2.91
// Based on NVVM 7.0.1
//

.version 8.2
.target sm_60
.address_size 64

	// .globl	bwdif_uchar
.global .align 4 .b8 coef_lf[8] = {213, 16, 0, 0, 213};
.global .align 4 .b8 coef_hf[12] = {194, 21, 0, 0, 217, 14, 0, 0, 248, 3};
.global .align 4 .b8 coef_sp[8] = {213, 19, 0, 0, 213, 3};

.visible .entry bwdif_uchar(
	.param .u64 bwdif_uchar_param_0,
	.param .u64 bwdif_uchar_param_1,
	.param .u64 bwdif_uchar_param_2,
	.param .u64 bwdif_uchar_param_3,
	.param .u32 bwdif_uchar_param_4,
	.param .u32 bwdif_uchar_param_5,
	.param .u32 bwdif_uchar_param_6,
	.param .u32 bwdif_uchar_param_7,
	.param .u32 bwdif_uchar_param_8,
	.param .u32 bwdif_uchar_param_9,
	.param .u32 bwdif_uchar_param_10,
	.param .u32 bwdif_uchar_param_11,
	.param .u32 bwdif_uchar_param_12
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<196>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [bwdif_uchar_param_0];
	ld.param.u64 	%rd3, [bwdif_uchar_param_1];
	ld.param.u64 	%rd4, [bwdif_uchar_param_2];
	ld.param.u64 	%rd5, [bwdif_uchar_param_3];
	ld.param.u32 	%r54, [bwdif_uchar_param_4];
	ld.param.u32 	%r55, [bwdif_uchar_param_5];
	ld.param.u32 	%r49, [bwdif_uchar_param_6];
	ld.param.u32 	%r50, [bwdif_uchar_param_9];
	ld.param.u32 	%r51, [bwdif_uchar_param_10];
	ld.param.u32 	%r52, [bwdif_uchar_param_11];
	ld.param.u32 	%r53, [bwdif_uchar_param_12];
	mov.u32 	%r56, %ntid.x;
	mov.u32 	%r57, %ctaid.x;
	mov.u32 	%r58, %tid.x;
	mad.lo.s32 	%r1, %r57, %r56, %r58;
	mov.u32 	%r59, %ntid.y;
	mov.u32 	%r60, %ctaid.y;
	mov.u32 	%r61, %tid.y;
	mad.lo.s32 	%r2, %r60, %r59, %r61;
	setp.ge.s32 	%p1, %r1, %r54;
	setp.ge.s32 	%p2, %r2, %r55;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_11;

	shr.u32 	%r62, %r2, 31;
	add.s32 	%r63, %r2, %r62;
	and.b32  	%r64, %r63, -2;
	sub.s32 	%r65, %r2, %r64;
	setp.eq.s32 	%p4, %r65, %r50;
	cvt.rn.f32.s32 	%f1, %r1;
	mad.lo.s32 	%r66, %r2, %r49, %r1;
	cvt.s64.s32 	%rd6, %r66;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd1, %rd7, %rd6;
	@%p4 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_2;

$L__BB0_10:
	cvt.rn.f32.s32 	%f11, %r2;
	tex.2d.v4.u32.f32 	{%r189, %r190, %r191, %r192}, [%rd4, {%f1, %f11}];
	st.global.u8 	[%rd1], %r189;
	bra.uni 	$L__BB0_11;

$L__BB0_2:
	add.s32 	%r67, %r2, 3;
	cvt.rn.f32.s32 	%f4, %r67;
	tex.2d.v4.u32.f32 	{%r3, %r68, %r69, %r70}, [%rd4, {%f1, %f4}];
	add.s32 	%r71, %r2, 1;
	cvt.rn.f32.s32 	%f2, %r71;
	tex.2d.v4.u32.f32 	{%r4, %r72, %r73, %r74}, [%rd4, {%f1, %f2}];
	add.s32 	%r75, %r2, -1;
	cvt.rn.f32.s32 	%f3, %r75;
	tex.2d.v4.u32.f32 	{%r5, %r76, %r77, %r78}, [%rd4, {%f1, %f3}];
	add.s32 	%r79, %r2, -3;
	cvt.rn.f32.s32 	%f5, %r79;
	tex.2d.v4.u32.f32 	{%r6, %r80, %r81, %r82}, [%rd4, {%f1, %f5}];
	setp.eq.s32 	%p5, %r52, 0;
	@%p5 bra 	$L__BB0_4;

	and.b32  	%r83, %r5, 255;
	and.b32  	%r84, %r4, 255;
	add.s32 	%r85, %r83, %r84;
	mul.lo.s32 	%r86, %r85, 5077;
	and.b32  	%r87, %r3, 255;
	and.b32  	%r88, %r6, 255;
	add.s32 	%r89, %r88, %r87;
	mad.lo.s32 	%r90, %r89, -981, %r86;
	shr.s32 	%r91, %r90, 13;
	setp.lt.s32 	%p6, %r90, 0;
	min.s32 	%r92, %r91, %r53;
	selp.b32 	%r93, 0, %r92, %p6;
	st.global.u8 	[%rd1], %r93;
	bra.uni 	$L__BB0_11;

$L__BB0_4:
	setp.eq.s32 	%p7, %r50, %r51;
	selp.b64 	%rd8, %rd4, %rd3, %p7;
	selp.b64 	%rd9, %rd5, %rd4, %p7;
	add.s32 	%r94, %r2, 4;
	cvt.rn.f32.s32 	%f6, %r94;
	tex.2d.v4.u32.f32 	{%r7, %r8, %r9, %r10}, [%rd3, {%f1, %f6}];
	add.s32 	%r95, %r2, 2;
	cvt.rn.f32.s32 	%f7, %r95;
	tex.2d.v4.u32.f32 	{%r11, %r12, %r13, %r14}, [%rd3, {%f1, %f7}];
	cvt.rn.f32.s32 	%f8, %r2;
	tex.2d.v4.u32.f32 	{%r96, %r97, %r98, %r99}, [%rd3, {%f1, %f8}];
	add.s32 	%r100, %r2, -2;
	cvt.rn.f32.s32 	%f9, %r100;
	tex.2d.v4.u32.f32 	{%r15, %r16, %r17, %r18}, [%rd3, {%f1, %f9}];
	add.s32 	%r101, %r2, -4;
	cvt.rn.f32.s32 	%f10, %r101;
	tex.2d.v4.u32.f32 	{%r19, %r20, %r21, %r22}, [%rd3, {%f1, %f10}];
	tex.2d.v4.u32.f32 	{%r102, %r103, %r104, %r105}, [%rd8, {%f1, %f2}];
	tex.2d.v4.u32.f32 	{%r106, %r107, %r108, %r109}, [%rd8, {%f1, %f3}];
	tex.2d.v4.u32.f32 	{%r110, %r111, %r112, %r113}, [%rd9, {%f1, %f2}];
	tex.2d.v4.u32.f32 	{%r114, %r115, %r116, %r117}, [%rd9, {%f1, %f3}];
	tex.2d.v4.u32.f32 	{%r23, %r24, %r25, %r26}, [%rd5, {%f1, %f7}];
	tex.2d.v4.u32.f32 	{%r118, %r119, %r120, %r121}, [%rd5, {%f1, %f8}];
	tex.2d.v4.u32.f32 	{%r27, %r28, %r29, %r30}, [%rd5, {%f1, %f9}];
	tex.2d.v4.u32.f32 	{%r31, %r32, %r33, %r34}, [%rd5, {%f1, %f10}];
	and.b32  	%r122, %r96, 255;
	and.b32  	%r123, %r118, 255;
	add.s32 	%r35, %r123, %r122;
	shr.u32 	%r195, %r35, 1;
	sub.s32 	%r124, %r122, %r123;
	abs.s32 	%r37, %r124;
	and.b32  	%r125, %r106, 255;
	and.b32  	%r38, %r5, 255;
	sub.s32 	%r126, %r125, %r38;
	abs.s32 	%r127, %r126;
	and.b32  	%r128, %r102, 255;
	and.b32  	%r39, %r4, 255;
	sub.s32 	%r129, %r128, %r39;
	abs.s32 	%r130, %r129;
	add.s32 	%r131, %r127, %r130;
	shr.u32 	%r132, %r131, 1;
	and.b32  	%r133, %r114, 255;
	sub.s32 	%r134, %r133, %r38;
	abs.s32 	%r135, %r134;
	and.b32  	%r136, %r110, 255;
	sub.s32 	%r137, %r136, %r39;
	abs.s32 	%r138, %r137;
	add.s32 	%r139, %r135, %r138;
	shr.u32 	%r140, %r139, 1;
	shr.u32 	%r141, %r37, 1;
	max.u32 	%r142, %r141, %r132;
	max.s32 	%r40, %r142, %r140;
	setp.eq.s32 	%p8, %r40, 0;
	@%p8 bra 	$L__BB0_9;

	and.b32  	%r143, %r15, 255;
	and.b32  	%r144, %r27, 255;
	add.s32 	%r41, %r144, %r143;
	and.b32  	%r145, %r11, 255;
	and.b32  	%r146, %r23, 255;
	add.s32 	%r42, %r146, %r145;
	sub.s32 	%r147, %r38, %r39;
	abs.s32 	%r148, %r147;
	setp.gt.s32 	%p9, %r148, %r37;
	@%p9 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_6;

$L__BB0_7:
	mul.lo.s32 	%r152, %r35, 5570;
	add.s32 	%r153, %r41, %r42;
	mad.lo.s32 	%r154, %r153, -3801, %r152;
	and.b32  	%r155, %r7, 255;
	and.b32  	%r156, %r19, 255;
	add.s32 	%r157, %r156, %r155;
	and.b32  	%r158, %r31, 255;
	add.s32 	%r159, %r157, %r158;
	add.s32 	%r160, %r159, %r158;
	mad.lo.s32 	%r161, %r160, 1016, %r154;
	shr.s32 	%r162, %r161, 2;
	add.s32 	%r163, %r38, %r39;
	mad.lo.s32 	%r193, %r163, 4309, %r162;
	mov.u32 	%r194, -213;
	bra.uni 	$L__BB0_8;

$L__BB0_6:
	add.s32 	%r150, %r38, %r39;
	mul.lo.s32 	%r193, %r150, 5077;
	mov.u32 	%r194, -981;

$L__BB0_8:
	shr.u32 	%r164, %r41, 1;
	sub.s32 	%r165, %r164, %r38;
	shr.u32 	%r166, %r42, 1;
	sub.s32 	%r167, %r166, %r39;
	min.s32 	%r168, %r165, %r167;
	sub.s32 	%r169, %r195, %r39;
	sub.s32 	%r170, %r195, %r38;
	max.s32 	%r171, %r169, %r170;
	max.s32 	%r172, %r165, %r167;
	min.s32 	%r173, %r169, %r170;
	min.s32 	%r174, %r173, %r172;
	max.s32 	%r175, %r171, %r168;
	neg.s32 	%r176, %r175;
	max.s32 	%r177, %r40, %r174;
	max.s32 	%r178, %r177, %r176;
	and.b32  	%r179, %r3, 255;
	and.b32  	%r180, %r6, 255;
	add.s32 	%r181, %r180, %r179;
	mad.lo.s32 	%r182, %r194, %r181, %r193;
	shr.s32 	%r183, %r182, 13;
	add.s32 	%r184, %r178, %r195;
	setp.gt.s32 	%p10, %r183, %r184;
	sub.s32 	%r185, %r195, %r178;
	max.s32 	%r186, %r183, %r185;
	selp.b32 	%r187, %r184, %r186, %p10;
	setp.lt.s32 	%p11, %r187, 0;
	min.s32 	%r188, %r187, %r53;
	selp.b32 	%r195, 0, %r188, %p11;

$L__BB0_9:
	st.global.u8 	[%rd1], %r195;

$L__BB0_11:
	ret;

}
	// .globl	bwdif_ushort
.visible .entry bwdif_ushort(
	.param .u64 bwdif_ushort_param_0,
	.param .u64 bwdif_ushort_param_1,
	.param .u64 bwdif_ushort_param_2,
	.param .u64 bwdif_ushort_param_3,
	.param .u32 bwdif_ushort_param_4,
	.param .u32 bwdif_ushort_param_5,
	.param .u32 bwdif_ushort_param_6,
	.param .u32 bwdif_ushort_param_7,
	.param .u32 bwdif_ushort_param_8,
	.param .u32 bwdif_ushort_param_9,
	.param .u32 bwdif_ushort_param_10,
	.param .u32 bwdif_ushort_param_11,
	.param .u32 bwdif_ushort_param_12
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<196>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [bwdif_ushort_param_0];
	ld.param.u64 	%rd3, [bwdif_ushort_param_1];
	ld.param.u64 	%rd4, [bwdif_ushort_param_2];
	ld.param.u64 	%rd5, [bwdif_ushort_param_3];
	ld.param.u32 	%r54, [bwdif_ushort_param_4];
	ld.param.u32 	%r55, [bwdif_ushort_param_5];
	ld.param.u32 	%r49, [bwdif_ushort_param_6];
	ld.param.u32 	%r50, [bwdif_ushort_param_9];
	ld.param.u32 	%r51, [bwdif_ushort_param_10];
	ld.param.u32 	%r52, [bwdif_ushort_param_11];
	ld.param.u32 	%r53, [bwdif_ushort_param_12];
	mov.u32 	%r56, %ntid.x;
	mov.u32 	%r57, %ctaid.x;
	mov.u32 	%r58, %tid.x;
	mad.lo.s32 	%r1, %r57, %r56, %r58;
	mov.u32 	%r59, %ntid.y;
	mov.u32 	%r60, %ctaid.y;
	mov.u32 	%r61, %tid.y;
	mad.lo.s32 	%r2, %r60, %r59, %r61;
	setp.ge.s32 	%p1, %r1, %r54;
	setp.ge.s32 	%p2, %r2, %r55;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB1_11;

	shr.u32 	%r62, %r2, 31;
	add.s32 	%r63, %r2, %r62;
	and.b32  	%r64, %r63, -2;
	sub.s32 	%r65, %r2, %r64;
	setp.eq.s32 	%p4, %r65, %r50;
	cvt.rn.f32.s32 	%f1, %r1;
	mad.lo.s32 	%r66, %r2, %r49, %r1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r66, 2;
	add.s64 	%rd1, %rd6, %rd7;
	@%p4 bra 	$L__BB1_10;
	bra.uni 	$L__BB1_2;

$L__BB1_10:
	cvt.rn.f32.s32 	%f11, %r2;
	tex.2d.v4.u32.f32 	{%r189, %r190, %r191, %r192}, [%rd4, {%f1, %f11}];
	st.global.u16 	[%rd1], %r189;
	bra.uni 	$L__BB1_11;

$L__BB1_2:
	add.s32 	%r67, %r2, 3;
	cvt.rn.f32.s32 	%f4, %r67;
	tex.2d.v4.u32.f32 	{%r3, %r68, %r69, %r70}, [%rd4, {%f1, %f4}];
	add.s32 	%r71, %r2, 1;
	cvt.rn.f32.s32 	%f2, %r71;
	tex.2d.v4.u32.f32 	{%r4, %r72, %r73, %r74}, [%rd4, {%f1, %f2}];
	add.s32 	%r75, %r2, -1;
	cvt.rn.f32.s32 	%f3, %r75;
	tex.2d.v4.u32.f32 	{%r5, %r76, %r77, %r78}, [%rd4, {%f1, %f3}];
	add.s32 	%r79, %r2, -3;
	cvt.rn.f32.s32 	%f5, %r79;
	tex.2d.v4.u32.f32 	{%r6, %r80, %r81, %r82}, [%rd4, {%f1, %f5}];
	setp.eq.s32 	%p5, %r52, 0;
	@%p5 bra 	$L__BB1_4;

	and.b32  	%r83, %r5, 65535;
	and.b32  	%r84, %r4, 65535;
	add.s32 	%r85, %r83, %r84;
	mul.lo.s32 	%r86, %r85, 5077;
	and.b32  	%r87, %r3, 65535;
	and.b32  	%r88, %r6, 65535;
	add.s32 	%r89, %r88, %r87;
	mad.lo.s32 	%r90, %r89, -981, %r86;
	shr.s32 	%r91, %r90, 13;
	setp.lt.s32 	%p6, %r90, 0;
	min.s32 	%r92, %r91, %r53;
	selp.b32 	%r93, 0, %r92, %p6;
	st.global.u16 	[%rd1], %r93;
	bra.uni 	$L__BB1_11;

$L__BB1_4:
	setp.eq.s32 	%p7, %r50, %r51;
	selp.b64 	%rd8, %rd4, %rd3, %p7;
	selp.b64 	%rd9, %rd5, %rd4, %p7;
	add.s32 	%r94, %r2, 4;
	cvt.rn.f32.s32 	%f6, %r94;
	tex.2d.v4.u32.f32 	{%r7, %r8, %r9, %r10}, [%rd3, {%f1, %f6}];
	add.s32 	%r95, %r2, 2;
	cvt.rn.f32.s32 	%f7, %r95;
	tex.2d.v4.u32.f32 	{%r11, %r12, %r13, %r14}, [%rd3, {%f1, %f7}];
	cvt.rn.f32.s32 	%f8, %r2;
	tex.2d.v4.u32.f32 	{%r96, %r97, %r98, %r99}, [%rd3, {%f1, %f8}];
	add.s32 	%r100, %r2, -2;
	cvt.rn.f32.s32 	%f9, %r100;
	tex.2d.v4.u32.f32 	{%r15, %r16, %r17, %r18}, [%rd3, {%f1, %f9}];
	add.s32 	%r101, %r2, -4;
	cvt.rn.f32.s32 	%f10, %r101;
	tex.2d.v4.u32.f32 	{%r19, %r20, %r21, %r22}, [%rd3, {%f1, %f10}];
	tex.2d.v4.u32.f32 	{%r102, %r103, %r104, %r105}, [%rd8, {%f1, %f2}];
	tex.2d.v4.u32.f32 	{%r106, %r107, %r108, %r109}, [%rd8, {%f1, %f3}];
	tex.2d.v4.u32.f32 	{%r110, %r111, %r112, %r113}, [%rd9, {%f1, %f2}];
	tex.2d.v4.u32.f32 	{%r114, %r115, %r116, %r117}, [%rd9, {%f1, %f3}];
	tex.2d.v4.u32.f32 	{%r23, %r24, %r25, %r26}, [%rd5, {%f1, %f7}];
	tex.2d.v4.u32.f32 	{%r118, %r119, %r120, %r121}, [%rd5, {%f1, %f8}];
	tex.2d.v4.u32.f32 	{%r27, %r28, %r29, %r30}, [%rd5, {%f1, %f9}];
	tex.2d.v4.u32.f32 	{%r31, %r32, %r33, %r34}, [%rd5, {%f1, %f10}];
	and.b32  	%r122, %r96, 65535;
	and.b32  	%r123, %r118, 65535;
	add.s32 	%r35, %r123, %r122;
	shr.u32 	%r195, %r35, 1;
	sub.s32 	%r124, %r122, %r123;
	abs.s32 	%r37, %r124;
	and.b32  	%r125, %r106, 65535;
	and.b32  	%r38, %r5, 65535;
	sub.s32 	%r126, %r125, %r38;
	abs.s32 	%r127, %r126;
	and.b32  	%r128, %r102, 65535;
	and.b32  	%r39, %r4, 65535;
	sub.s32 	%r129, %r128, %r39;
	abs.s32 	%r130, %r129;
	add.s32 	%r131, %r127, %r130;
	shr.u32 	%r132, %r131, 1;
	and.b32  	%r133, %r114, 65535;
	sub.s32 	%r134, %r133, %r38;
	abs.s32 	%r135, %r134;
	and.b32  	%r136, %r110, 65535;
	sub.s32 	%r137, %r136, %r39;
	abs.s32 	%r138, %r137;
	add.s32 	%r139, %r135, %r138;
	shr.u32 	%r140, %r139, 1;
	shr.u32 	%r141, %r37, 1;
	max.u32 	%r142, %r141, %r132;
	max.s32 	%r40, %r142, %r140;
	setp.eq.s32 	%p8, %r40, 0;
	@%p8 bra 	$L__BB1_9;

	and.b32  	%r143, %r15, 65535;
	and.b32  	%r144, %r27, 65535;
	add.s32 	%r41, %r144, %r143;
	and.b32  	%r145, %r11, 65535;
	and.b32  	%r146, %r23, 65535;
	add.s32 	%r42, %r146, %r145;
	sub.s32 	%r147, %r38, %r39;
	abs.s32 	%r148, %r147;
	setp.gt.s32 	%p9, %r148, %r37;
	@%p9 bra 	$L__BB1_7;
	bra.uni 	$L__BB1_6;

$L__BB1_7:
	mul.lo.s32 	%r152, %r35, 5570;
	add.s32 	%r153, %r41, %r42;
	mad.lo.s32 	%r154, %r153, -3801, %r152;
	and.b32  	%r155, %r7, 65535;
	and.b32  	%r156, %r19, 65535;
	add.s32 	%r157, %r156, %r155;
	and.b32  	%r158, %r31, 65535;
	add.s32 	%r159, %r157, %r158;
	add.s32 	%r160, %r159, %r158;
	mad.lo.s32 	%r161, %r160, 1016, %r154;
	shr.s32 	%r162, %r161, 2;
	add.s32 	%r163, %r38, %r39;
	mad.lo.s32 	%r193, %r163, 4309, %r162;
	mov.u32 	%r194, -213;
	bra.uni 	$L__BB1_8;

$L__BB1_6:
	add.s32 	%r150, %r38, %r39;
	mul.lo.s32 	%r193, %r150, 5077;
	mov.u32 	%r194, -981;

$L__BB1_8:
	shr.u32 	%r164, %r41, 1;
	sub.s32 	%r165, %r164, %r38;
	shr.u32 	%r166, %r42, 1;
	sub.s32 	%r167, %r166, %r39;
	min.s32 	%r168, %r165, %r167;
	sub.s32 	%r169, %r195, %r39;
	sub.s32 	%r170, %r195, %r38;
	max.s32 	%r171, %r169, %r170;
	max.s32 	%r172, %r165, %r167;
	min.s32 	%r173, %r169, %r170;
	min.s32 	%r174, %r173, %r172;
	max.s32 	%r175, %r171, %r168;
	neg.s32 	%r176, %r175;
	max.s32 	%r177, %r40, %r174;
	max.s32 	%r178, %r177, %r176;
	and.b32  	%r179, %r3, 65535;
	and.b32  	%r180, %r6, 65535;
	add.s32 	%r181, %r180, %r179;
	mad.lo.s32 	%r182, %r194, %r181, %r193;
	shr.s32 	%r183, %r182, 13;
	add.s32 	%r184, %r178, %r195;
	setp.gt.s32 	%p10, %r183, %r184;
	sub.s32 	%r185, %r195, %r178;
	max.s32 	%r186, %r183, %r185;
	selp.b32 	%r187, %r184, %r186, %p10;
	setp.lt.s32 	%p11, %r187, 0;
	min.s32 	%r188, %r187, %r53;
	selp.b32 	%r195, 0, %r188, %p11;

$L__BB1_9:
	st.global.u16 	[%rd1], %r195;

$L__BB1_11:
	ret;

}
	// .globl	bwdif_uchar2
.visible .entry bwdif_uchar2(
	.param .u64 bwdif_uchar2_param_0,
	.param .u64 bwdif_uchar2_param_1,
	.param .u64 bwdif_uchar2_param_2,
	.param .u64 bwdif_uchar2_param_3,
	.param .u32 bwdif_uchar2_param_4,
	.param .u32 bwdif_uchar2_param_5,
	.param .u32 bwdif_uchar2_param_6,
	.param .u32 bwdif_uchar2_param_7,
	.param .u32 bwdif_uchar2_param_8,
	.param .u32 bwdif_uchar2_param_9,
	.param .u32 bwdif_uchar2_param_10,
	.param .u32 bwdif_uchar2_param_11,
	.param .u32 bwdif_uchar2_param_12
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<291>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [bwdif_uchar2_param_0];
	ld.param.u64 	%rd3, [bwdif_uchar2_param_1];
	ld.param.u64 	%rd4, [bwdif_uchar2_param_2];
	ld.param.u64 	%rd5, [bwdif_uchar2_param_3];
	ld.param.u32 	%r96, [bwdif_uchar2_param_4];
	ld.param.u32 	%r97, [bwdif_uchar2_param_5];
	ld.param.u32 	%r91, [bwdif_uchar2_param_6];
	ld.param.u32 	%r92, [bwdif_uchar2_param_9];
	ld.param.u32 	%r93, [bwdif_uchar2_param_10];
	ld.param.u32 	%r94, [bwdif_uchar2_param_11];
	ld.param.u32 	%r95, [bwdif_uchar2_param_12];
	mov.u32 	%r98, %ntid.x;
	mov.u32 	%r99, %ctaid.x;
	mov.u32 	%r100, %tid.x;
	mad.lo.s32 	%r1, %r99, %r98, %r100;
	mov.u32 	%r101, %ntid.y;
	mov.u32 	%r102, %ctaid.y;
	mov.u32 	%r103, %tid.y;
	mad.lo.s32 	%r2, %r102, %r101, %r103;
	setp.ge.s32 	%p1, %r1, %r96;
	setp.ge.s32 	%p2, %r2, %r97;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB2_16;

	shr.u32 	%r104, %r2, 31;
	add.s32 	%r105, %r2, %r104;
	and.b32  	%r106, %r105, -2;
	sub.s32 	%r107, %r2, %r106;
	setp.eq.s32 	%p4, %r107, %r92;
	mad.lo.s32 	%r108, %r2, %r91, %r1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r108, 2;
	add.s64 	%rd1, %rd6, %rd7;
	@%p4 bra 	$L__BB2_15;
	bra.uni 	$L__BB2_2;

$L__BB2_15:
	cvt.rn.f32.s32 	%f11, %r1;
	cvt.rn.f32.s32 	%f12, %r2;
	tex.2d.v4.u32.f32 	{%r281, %r282, %r283, %r284}, [%rd4, {%f11, %f12}];
	cvt.u16.u32 	%rs5, %r282;
	cvt.u16.u32 	%rs6, %r281;
	st.global.v2.u8 	[%rd1], {%rs6, %rs5};
	bra.uni 	$L__BB2_16;

$L__BB2_2:
	cvt.rn.f32.s32 	%f1, %r1;
	add.s32 	%r109, %r2, 3;
	cvt.rn.f32.s32 	%f4, %r109;
	tex.2d.v4.u32.f32 	{%r3, %r4, %r110, %r111}, [%rd4, {%f1, %f4}];
	add.s32 	%r112, %r2, 1;
	cvt.rn.f32.s32 	%f2, %r112;
	tex.2d.v4.u32.f32 	{%r5, %r6, %r113, %r114}, [%rd4, {%f1, %f2}];
	add.s32 	%r115, %r2, -1;
	cvt.rn.f32.s32 	%f3, %r115;
	tex.2d.v4.u32.f32 	{%r7, %r8, %r116, %r117}, [%rd4, {%f1, %f3}];
	add.s32 	%r118, %r2, -3;
	cvt.rn.f32.s32 	%f5, %r118;
	tex.2d.v4.u32.f32 	{%r9, %r10, %r119, %r120}, [%rd4, {%f1, %f5}];
	setp.eq.s32 	%p5, %r94, 0;
	@%p5 bra 	$L__BB2_4;

	and.b32  	%r121, %r7, 255;
	and.b32  	%r122, %r5, 255;
	add.s32 	%r123, %r121, %r122;
	mul.lo.s32 	%r124, %r123, 5077;
	and.b32  	%r125, %r3, 255;
	and.b32  	%r126, %r9, 255;
	add.s32 	%r127, %r126, %r125;
	mad.lo.s32 	%r128, %r127, -981, %r124;
	shr.s32 	%r129, %r128, 13;
	setp.lt.s32 	%p6, %r128, 0;
	min.s32 	%r130, %r129, %r95;
	selp.b32 	%r131, 0, %r130, %p6;
	and.b32  	%r132, %r6, 255;
	and.b32  	%r133, %r8, 255;
	add.s32 	%r134, %r133, %r132;
	mul.lo.s32 	%r135, %r134, 5077;
	and.b32  	%r136, %r4, 255;
	and.b32  	%r137, %r10, 255;
	add.s32 	%r138, %r137, %r136;
	mad.lo.s32 	%r139, %r138, -981, %r135;
	shr.s32 	%r140, %r139, 13;
	setp.lt.s32 	%p7, %r139, 0;
	min.s32 	%r141, %r140, %r95;
	selp.b32 	%r142, 0, %r141, %p7;
	cvt.u16.u32 	%rs1, %r142;
	cvt.u16.u32 	%rs2, %r131;
	st.global.v2.u8 	[%rd1], {%rs2, %rs1};
	bra.uni 	$L__BB2_16;

$L__BB2_4:
	setp.eq.s32 	%p8, %r92, %r93;
	selp.b64 	%rd8, %rd4, %rd3, %p8;
	selp.b64 	%rd9, %rd5, %rd4, %p8;
	add.s32 	%r143, %r2, 4;
	cvt.rn.f32.s32 	%f6, %r143;
	tex.2d.v4.u32.f32 	{%r11, %r12, %r13, %r14}, [%rd3, {%f1, %f6}];
	add.s32 	%r144, %r2, 2;
	cvt.rn.f32.s32 	%f7, %r144;
	tex.2d.v4.u32.f32 	{%r15, %r16, %r17, %r18}, [%rd3, {%f1, %f7}];
	cvt.rn.f32.s32 	%f8, %r2;
	tex.2d.v4.u32.f32 	{%r19, %r20, %r21, %r22}, [%rd3, {%f1, %f8}];
	add.s32 	%r145, %r2, -2;
	cvt.rn.f32.s32 	%f9, %r145;
	tex.2d.v4.u32.f32 	{%r23, %r24, %r25, %r26}, [%rd3, {%f1, %f9}];
	add.s32 	%r146, %r2, -4;
	cvt.rn.f32.s32 	%f10, %r146;
	tex.2d.v4.u32.f32 	{%r27, %r28, %r29, %r30}, [%rd3, {%f1, %f10}];
	tex.2d.v4.u32.f32 	{%r31, %r32, %r33, %r34}, [%rd8, {%f1, %f2}];
	tex.2d.v4.u32.f32 	{%r35, %r36, %r37, %r38}, [%rd8, {%f1, %f3}];
	tex.2d.v4.u32.f32 	{%r39, %r40, %r41, %r42}, [%rd9, {%f1, %f2}];
	tex.2d.v4.u32.f32 	{%r43, %r44, %r45, %r46}, [%rd9, {%f1, %f3}];
	tex.2d.v4.u32.f32 	{%r47, %r48, %r49, %r50}, [%rd5, {%f1, %f7}];
	tex.2d.v4.u32.f32 	{%r51, %r52, %r53, %r54}, [%rd5, {%f1, %f8}];
	tex.2d.v4.u32.f32 	{%r55, %r56, %r57, %r58}, [%rd5, {%f1, %f9}];
	tex.2d.v4.u32.f32 	{%r59, %r60, %r61, %r62}, [%rd5, {%f1, %f10}];
	and.b32  	%r147, %r19, 255;
	and.b32  	%r148, %r51, 255;
	add.s32 	%r63, %r148, %r147;
	shr.u32 	%r287, %r63, 1;
	sub.s32 	%r149, %r147, %r148;
	abs.s32 	%r65, %r149;
	and.b32  	%r150, %r35, 255;
	and.b32  	%r66, %r7, 255;
	sub.s32 	%r151, %r150, %r66;
	abs.s32 	%r152, %r151;
	and.b32  	%r153, %r31, 255;
	and.b32  	%r67, %r5, 255;
	sub.s32 	%r154, %r153, %r67;
	abs.s32 	%r155, %r154;
	add.s32 	%r156, %r152, %r155;
	shr.u32 	%r157, %r156, 1;
	and.b32  	%r158, %r43, 255;
	sub.s32 	%r159, %r158, %r66;
	abs.s32 	%r160, %r159;
	and.b32  	%r161, %r39, 255;
	sub.s32 	%r162, %r161, %r67;
	abs.s32 	%r163, %r162;
	add.s32 	%r164, %r160, %r163;
	shr.u32 	%r165, %r164, 1;
	shr.u32 	%r166, %r65, 1;
	max.u32 	%r167, %r166, %r157;
	max.s32 	%r68, %r167, %r165;
	setp.eq.s32 	%p9, %r68, 0;
	@%p9 bra 	$L__BB2_9;

	and.b32  	%r168, %r23, 255;
	and.b32  	%r169, %r55, 255;
	add.s32 	%r69, %r169, %r168;
	and.b32  	%r170, %r15, 255;
	and.b32  	%r171, %r47, 255;
	add.s32 	%r70, %r171, %r170;
	sub.s32 	%r172, %r66, %r67;
	abs.s32 	%r173, %r172;
	setp.gt.s32 	%p10, %r173, %r65;
	@%p10 bra 	$L__BB2_7;
	bra.uni 	$L__BB2_6;

$L__BB2_7:
	mul.lo.s32 	%r177, %r63, 5570;
	add.s32 	%r178, %r69, %r70;
	mad.lo.s32 	%r179, %r178, -3801, %r177;
	and.b32  	%r180, %r11, 255;
	and.b32  	%r181, %r27, 255;
	add.s32 	%r182, %r181, %r180;
	and.b32  	%r183, %r59, 255;
	add.s32 	%r184, %r182, %r183;
	add.s32 	%r185, %r184, %r183;
	mad.lo.s32 	%r186, %r185, 1016, %r179;
	shr.s32 	%r187, %r186, 2;
	add.s32 	%r188, %r66, %r67;
	mad.lo.s32 	%r285, %r188, 4309, %r187;
	mov.u32 	%r286, -213;
	bra.uni 	$L__BB2_8;

$L__BB2_6:
	add.s32 	%r175, %r66, %r67;
	mul.lo.s32 	%r285, %r175, 5077;
	mov.u32 	%r286, -981;

$L__BB2_8:
	shr.u32 	%r189, %r69, 1;
	sub.s32 	%r190, %r189, %r66;
	shr.u32 	%r191, %r70, 1;
	sub.s32 	%r192, %r191, %r67;
	min.s32 	%r193, %r190, %r192;
	sub.s32 	%r194, %r287, %r67;
	sub.s32 	%r195, %r287, %r66;
	max.s32 	%r196, %r194, %r195;
	max.s32 	%r197, %r190, %r192;
	min.s32 	%r198, %r194, %r195;
	min.s32 	%r199, %r198, %r197;
	max.s32 	%r200, %r196, %r193;
	neg.s32 	%r201, %r200;
	max.s32 	%r202, %r68, %r199;
	max.s32 	%r203, %r202, %r201;
	and.b32  	%r204, %r3, 255;
	and.b32  	%r205, %r9, 255;
	add.s32 	%r206, %r205, %r204;
	mad.lo.s32 	%r207, %r286, %r206, %r285;
	shr.s32 	%r208, %r207, 13;
	add.s32 	%r209, %r203, %r287;
	setp.gt.s32 	%p11, %r208, %r209;
	sub.s32 	%r210, %r287, %r203;
	max.s32 	%r211, %r208, %r210;
	selp.b32 	%r212, %r209, %r211, %p11;
	setp.lt.s32 	%p12, %r212, 0;
	min.s32 	%r213, %r212, %r95;
	selp.b32 	%r287, 0, %r213, %p12;

$L__BB2_9:
	and.b32  	%r214, %r52, 255;
	and.b32  	%r215, %r20, 255;
	add.s32 	%r77, %r214, %r215;
	shr.u32 	%r290, %r77, 1;
	sub.s32 	%r216, %r215, %r214;
	abs.s32 	%r79, %r216;
	and.b32  	%r217, %r36, 255;
	and.b32  	%r80, %r8, 255;
	sub.s32 	%r218, %r217, %r80;
	abs.s32 	%r219, %r218;
	and.b32  	%r220, %r32, 255;
	and.b32  	%r81, %r6, 255;
	sub.s32 	%r221, %r220, %r81;
	abs.s32 	%r222, %r221;
	add.s32 	%r223, %r219, %r222;
	shr.u32 	%r224, %r223, 1;
	and.b32  	%r225, %r44, 255;
	sub.s32 	%r226, %r225, %r80;
	abs.s32 	%r227, %r226;
	and.b32  	%r228, %r40, 255;
	sub.s32 	%r229, %r228, %r81;
	abs.s32 	%r230, %r229;
	add.s32 	%r231, %r227, %r230;
	shr.u32 	%r232, %r231, 1;
	shr.u32 	%r233, %r79, 1;
	max.u32 	%r234, %r233, %r224;
	max.s32 	%r82, %r234, %r232;
	setp.eq.s32 	%p13, %r82, 0;
	@%p13 bra 	$L__BB2_14;

	and.b32  	%r235, %r24, 255;
	and.b32  	%r236, %r56, 255;
	add.s32 	%r83, %r236, %r235;
	and.b32  	%r237, %r48, 255;
	and.b32  	%r238, %r16, 255;
	add.s32 	%r84, %r237, %r238;
	sub.s32 	%r239, %r80, %r81;
	abs.s32 	%r240, %r239;
	setp.gt.s32 	%p14, %r240, %r79;
	@%p14 bra 	$L__BB2_12;
	bra.uni 	$L__BB2_11;

$L__BB2_12:
	mul.lo.s32 	%r244, %r77, 5570;
	add.s32 	%r245, %r83, %r84;
	mad.lo.s32 	%r246, %r245, -3801, %r244;
	and.b32  	%r247, %r12, 255;
	and.b32  	%r248, %r28, 255;
	add.s32 	%r249, %r248, %r247;
	and.b32  	%r250, %r60, 255;
	add.s32 	%r251, %r249, %r250;
	add.s32 	%r252, %r251, %r250;
	mad.lo.s32 	%r253, %r252, 1016, %r246;
	shr.s32 	%r254, %r253, 2;
	add.s32 	%r255, %r80, %r81;
	mad.lo.s32 	%r288, %r255, 4309, %r254;
	mov.u32 	%r289, -213;
	bra.uni 	$L__BB2_13;

$L__BB2_11:
	add.s32 	%r242, %r80, %r81;
	mul.lo.s32 	%r288, %r242, 5077;
	mov.u32 	%r289, -981;

$L__BB2_13:
	shr.u32 	%r256, %r83, 1;
	sub.s32 	%r257, %r256, %r80;
	shr.u32 	%r258, %r84, 1;
	sub.s32 	%r259, %r258, %r81;
	min.s32 	%r260, %r257, %r259;
	sub.s32 	%r261, %r290, %r81;
	sub.s32 	%r262, %r290, %r80;
	max.s32 	%r263, %r261, %r262;
	max.s32 	%r264, %r257, %r259;
	min.s32 	%r265, %r261, %r262;
	min.s32 	%r266, %r265, %r264;
	max.s32 	%r267, %r263, %r260;
	neg.s32 	%r268, %r267;
	max.s32 	%r269, %r82, %r266;
	max.s32 	%r270, %r269, %r268;
	and.b32  	%r271, %r4, 255;
	and.b32  	%r272, %r10, 255;
	add.s32 	%r273, %r272, %r271;
	mad.lo.s32 	%r274, %r289, %r273, %r288;
	shr.s32 	%r275, %r274, 13;
	add.s32 	%r276, %r270, %r290;
	setp.gt.s32 	%p15, %r275, %r276;
	sub.s32 	%r277, %r290, %r270;
	max.s32 	%r278, %r275, %r277;
	selp.b32 	%r279, %r276, %r278, %p15;
	setp.lt.s32 	%p16, %r279, 0;
	min.s32 	%r280, %r279, %r95;
	selp.b32 	%r290, 0, %r280, %p16;

$L__BB2_14:
	cvt.u16.u32 	%rs3, %r290;
	cvt.u16.u32 	%rs4, %r287;
	st.global.v2.u8 	[%rd1], {%rs4, %rs3};

$L__BB2_16:
	ret;

}
	// .globl	bwdif_ushort2
.visible .entry bwdif_ushort2(
	.param .u64 bwdif_ushort2_param_0,
	.param .u64 bwdif_ushort2_param_1,
	.param .u64 bwdif_ushort2_param_2,
	.param .u64 bwdif_ushort2_param_3,
	.param .u32 bwdif_ushort2_param_4,
	.param .u32 bwdif_ushort2_param_5,
	.param .u32 bwdif_ushort2_param_6,
	.param .u32 bwdif_ushort2_param_7,
	.param .u32 bwdif_ushort2_param_8,
	.param .u32 bwdif_ushort2_param_9,
	.param .u32 bwdif_ushort2_param_10,
	.param .u32 bwdif_ushort2_param_11,
	.param .u32 bwdif_ushort2_param_12
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<291>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [bwdif_ushort2_param_0];
	ld.param.u64 	%rd3, [bwdif_ushort2_param_1];
	ld.param.u64 	%rd4, [bwdif_ushort2_param_2];
	ld.param.u64 	%rd5, [bwdif_ushort2_param_3];
	ld.param.u32 	%r96, [bwdif_ushort2_param_4];
	ld.param.u32 	%r97, [bwdif_ushort2_param_5];
	ld.param.u32 	%r91, [bwdif_ushort2_param_6];
	ld.param.u32 	%r92, [bwdif_ushort2_param_9];
	ld.param.u32 	%r93, [bwdif_ushort2_param_10];
	ld.param.u32 	%r94, [bwdif_ushort2_param_11];
	ld.param.u32 	%r95, [bwdif_ushort2_param_12];
	mov.u32 	%r98, %ntid.x;
	mov.u32 	%r99, %ctaid.x;
	mov.u32 	%r100, %tid.x;
	mad.lo.s32 	%r1, %r99, %r98, %r100;
	mov.u32 	%r101, %ntid.y;
	mov.u32 	%r102, %ctaid.y;
	mov.u32 	%r103, %tid.y;
	mad.lo.s32 	%r2, %r102, %r101, %r103;
	setp.ge.s32 	%p1, %r1, %r96;
	setp.ge.s32 	%p2, %r2, %r97;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB3_16;

	shr.u32 	%r104, %r2, 31;
	add.s32 	%r105, %r2, %r104;
	and.b32  	%r106, %r105, -2;
	sub.s32 	%r107, %r2, %r106;
	setp.eq.s32 	%p4, %r107, %r92;
	mad.lo.s32 	%r108, %r2, %r91, %r1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r108, 4;
	add.s64 	%rd1, %rd6, %rd7;
	@%p4 bra 	$L__BB3_15;
	bra.uni 	$L__BB3_2;

$L__BB3_15:
	cvt.rn.f32.s32 	%f11, %r1;
	cvt.rn.f32.s32 	%f12, %r2;
	tex.2d.v4.u32.f32 	{%r281, %r282, %r283, %r284}, [%rd4, {%f11, %f12}];
	cvt.u16.u32 	%rs5, %r282;
	cvt.u16.u32 	%rs6, %r281;
	st.global.v2.u16 	[%rd1], {%rs6, %rs5};
	bra.uni 	$L__BB3_16;

$L__BB3_2:
	cvt.rn.f32.s32 	%f1, %r1;
	add.s32 	%r109, %r2, 3;
	cvt.rn.f32.s32 	%f4, %r109;
	tex.2d.v4.u32.f32 	{%r3, %r4, %r110, %r111}, [%rd4, {%f1, %f4}];
	add.s32 	%r112, %r2, 1;
	cvt.rn.f32.s32 	%f2, %r112;
	tex.2d.v4.u32.f32 	{%r5, %r6, %r113, %r114}, [%rd4, {%f1, %f2}];
	add.s32 	%r115, %r2, -1;
	cvt.rn.f32.s32 	%f3, %r115;
	tex.2d.v4.u32.f32 	{%r7, %r8, %r116, %r117}, [%rd4, {%f1, %f3}];
	add.s32 	%r118, %r2, -3;
	cvt.rn.f32.s32 	%f5, %r118;
	tex.2d.v4.u32.f32 	{%r9, %r10, %r119, %r120}, [%rd4, {%f1, %f5}];
	setp.eq.s32 	%p5, %r94, 0;
	@%p5 bra 	$L__BB3_4;

	and.b32  	%r121, %r7, 65535;
	and.b32  	%r122, %r5, 65535;
	add.s32 	%r123, %r121, %r122;
	mul.lo.s32 	%r124, %r123, 5077;
	and.b32  	%r125, %r3, 65535;
	and.b32  	%r126, %r9, 65535;
	add.s32 	%r127, %r126, %r125;
	mad.lo.s32 	%r128, %r127, -981, %r124;
	shr.s32 	%r129, %r128, 13;
	setp.lt.s32 	%p6, %r128, 0;
	min.s32 	%r130, %r129, %r95;
	selp.b32 	%r131, 0, %r130, %p6;
	and.b32  	%r132, %r6, 65535;
	and.b32  	%r133, %r8, 65535;
	add.s32 	%r134, %r133, %r132;
	mul.lo.s32 	%r135, %r134, 5077;
	and.b32  	%r136, %r4, 65535;
	and.b32  	%r137, %r10, 65535;
	add.s32 	%r138, %r137, %r136;
	mad.lo.s32 	%r139, %r138, -981, %r135;
	shr.s32 	%r140, %r139, 13;
	setp.lt.s32 	%p7, %r139, 0;
	min.s32 	%r141, %r140, %r95;
	selp.b32 	%r142, 0, %r141, %p7;
	cvt.u16.u32 	%rs1, %r142;
	cvt.u16.u32 	%rs2, %r131;
	st.global.v2.u16 	[%rd1], {%rs2, %rs1};
	bra.uni 	$L__BB3_16;

$L__BB3_4:
	setp.eq.s32 	%p8, %r92, %r93;
	selp.b64 	%rd8, %rd4, %rd3, %p8;
	selp.b64 	%rd9, %rd5, %rd4, %p8;
	add.s32 	%r143, %r2, 4;
	cvt.rn.f32.s32 	%f6, %r143;
	tex.2d.v4.u32.f32 	{%r11, %r12, %r13, %r14}, [%rd3, {%f1, %f6}];
	add.s32 	%r144, %r2, 2;
	cvt.rn.f32.s32 	%f7, %r144;
	tex.2d.v4.u32.f32 	{%r15, %r16, %r17, %r18}, [%rd3, {%f1, %f7}];
	cvt.rn.f32.s32 	%f8, %r2;
	tex.2d.v4.u32.f32 	{%r19, %r20, %r21, %r22}, [%rd3, {%f1, %f8}];
	add.s32 	%r145, %r2, -2;
	cvt.rn.f32.s32 	%f9, %r145;
	tex.2d.v4.u32.f32 	{%r23, %r24, %r25, %r26}, [%rd3, {%f1, %f9}];
	add.s32 	%r146, %r2, -4;
	cvt.rn.f32.s32 	%f10, %r146;
	tex.2d.v4.u32.f32 	{%r27, %r28, %r29, %r30}, [%rd3, {%f1, %f10}];
	tex.2d.v4.u32.f32 	{%r31, %r32, %r33, %r34}, [%rd8, {%f1, %f2}];
	tex.2d.v4.u32.f32 	{%r35, %r36, %r37, %r38}, [%rd8, {%f1, %f3}];
	tex.2d.v4.u32.f32 	{%r39, %r40, %r41, %r42}, [%rd9, {%f1, %f2}];
	tex.2d.v4.u32.f32 	{%r43, %r44, %r45, %r46}, [%rd9, {%f1, %f3}];
	tex.2d.v4.u32.f32 	{%r47, %r48, %r49, %r50}, [%rd5, {%f1, %f7}];
	tex.2d.v4.u32.f32 	{%r51, %r52, %r53, %r54}, [%rd5, {%f1, %f8}];
	tex.2d.v4.u32.f32 	{%r55, %r56, %r57, %r58}, [%rd5, {%f1, %f9}];
	tex.2d.v4.u32.f32 	{%r59, %r60, %r61, %r62}, [%rd5, {%f1, %f10}];
	and.b32  	%r147, %r19, 65535;
	and.b32  	%r148, %r51, 65535;
	add.s32 	%r63, %r148, %r147;
	shr.u32 	%r287, %r63, 1;
	sub.s32 	%r149, %r147, %r148;
	abs.s32 	%r65, %r149;
	and.b32  	%r150, %r35, 65535;
	and.b32  	%r66, %r7, 65535;
	sub.s32 	%r151, %r150, %r66;
	abs.s32 	%r152, %r151;
	and.b32  	%r153, %r31, 65535;
	and.b32  	%r67, %r5, 65535;
	sub.s32 	%r154, %r153, %r67;
	abs.s32 	%r155, %r154;
	add.s32 	%r156, %r152, %r155;
	shr.u32 	%r157, %r156, 1;
	and.b32  	%r158, %r43, 65535;
	sub.s32 	%r159, %r158, %r66;
	abs.s32 	%r160, %r159;
	and.b32  	%r161, %r39, 65535;
	sub.s32 	%r162, %r161, %r67;
	abs.s32 	%r163, %r162;
	add.s32 	%r164, %r160, %r163;
	shr.u32 	%r165, %r164, 1;
	shr.u32 	%r166, %r65, 1;
	max.u32 	%r167, %r166, %r157;
	max.s32 	%r68, %r167, %r165;
	setp.eq.s32 	%p9, %r68, 0;
	@%p9 bra 	$L__BB3_9;

	and.b32  	%r168, %r23, 65535;
	and.b32  	%r169, %r55, 65535;
	add.s32 	%r69, %r169, %r168;
	and.b32  	%r170, %r15, 65535;
	and.b32  	%r171, %r47, 65535;
	add.s32 	%r70, %r171, %r170;
	sub.s32 	%r172, %r66, %r67;
	abs.s32 	%r173, %r172;
	setp.gt.s32 	%p10, %r173, %r65;
	@%p10 bra 	$L__BB3_7;
	bra.uni 	$L__BB3_6;

$L__BB3_7:
	mul.lo.s32 	%r177, %r63, 5570;
	add.s32 	%r178, %r69, %r70;
	mad.lo.s32 	%r179, %r178, -3801, %r177;
	and.b32  	%r180, %r11, 65535;
	and.b32  	%r181, %r27, 65535;
	add.s32 	%r182, %r181, %r180;
	and.b32  	%r183, %r59, 65535;
	add.s32 	%r184, %r182, %r183;
	add.s32 	%r185, %r184, %r183;
	mad.lo.s32 	%r186, %r185, 1016, %r179;
	shr.s32 	%r187, %r186, 2;
	add.s32 	%r188, %r66, %r67;
	mad.lo.s32 	%r285, %r188, 4309, %r187;
	mov.u32 	%r286, -213;
	bra.uni 	$L__BB3_8;

$L__BB3_6:
	add.s32 	%r175, %r66, %r67;
	mul.lo.s32 	%r285, %r175, 5077;
	mov.u32 	%r286, -981;

$L__BB3_8:
	shr.u32 	%r189, %r69, 1;
	sub.s32 	%r190, %r189, %r66;
	shr.u32 	%r191, %r70, 1;
	sub.s32 	%r192, %r191, %r67;
	min.s32 	%r193, %r190, %r192;
	sub.s32 	%r194, %r287, %r67;
	sub.s32 	%r195, %r287, %r66;
	max.s32 	%r196, %r194, %r195;
	max.s32 	%r197, %r190, %r192;
	min.s32 	%r198, %r194, %r195;
	min.s32 	%r199, %r198, %r197;
	max.s32 	%r200, %r196, %r193;
	neg.s32 	%r201, %r200;
	max.s32 	%r202, %r68, %r199;
	max.s32 	%r203, %r202, %r201;
	and.b32  	%r204, %r3, 65535;
	and.b32  	%r205, %r9, 65535;
	add.s32 	%r206, %r205, %r204;
	mad.lo.s32 	%r207, %r286, %r206, %r285;
	shr.s32 	%r208, %r207, 13;
	add.s32 	%r209, %r203, %r287;
	setp.gt.s32 	%p11, %r208, %r209;
	sub.s32 	%r210, %r287, %r203;
	max.s32 	%r211, %r208, %r210;
	selp.b32 	%r212, %r209, %r211, %p11;
	setp.lt.s32 	%p12, %r212, 0;
	min.s32 	%r213, %r212, %r95;
	selp.b32 	%r287, 0, %r213, %p12;

$L__BB3_9:
	and.b32  	%r214, %r52, 65535;
	and.b32  	%r215, %r20, 65535;
	add.s32 	%r77, %r214, %r215;
	shr.u32 	%r290, %r77, 1;
	sub.s32 	%r216, %r215, %r214;
	abs.s32 	%r79, %r216;
	and.b32  	%r217, %r36, 65535;
	and.b32  	%r80, %r8, 65535;
	sub.s32 	%r218, %r217, %r80;
	abs.s32 	%r219, %r218;
	and.b32  	%r220, %r32, 65535;
	and.b32  	%r81, %r6, 65535;
	sub.s32 	%r221, %r220, %r81;
	abs.s32 	%r222, %r221;
	add.s32 	%r223, %r219, %r222;
	shr.u32 	%r224, %r223, 1;
	and.b32  	%r225, %r44, 65535;
	sub.s32 	%r226, %r225, %r80;
	abs.s32 	%r227, %r226;
	and.b32  	%r228, %r40, 65535;
	sub.s32 	%r229, %r228, %r81;
	abs.s32 	%r230, %r229;
	add.s32 	%r231, %r227, %r230;
	shr.u32 	%r232, %r231, 1;
	shr.u32 	%r233, %r79, 1;
	max.u32 	%r234, %r233, %r224;
	max.s32 	%r82, %r234, %r232;
	setp.eq.s32 	%p13, %r82, 0;
	@%p13 bra 	$L__BB3_14;

	and.b32  	%r235, %r24, 65535;
	and.b32  	%r236, %r56, 65535;
	add.s32 	%r83, %r236, %r235;
	and.b32  	%r237, %r48, 65535;
	and.b32  	%r238, %r16, 65535;
	add.s32 	%r84, %r237, %r238;
	sub.s32 	%r239, %r80, %r81;
	abs.s32 	%r240, %r239;
	setp.gt.s32 	%p14, %r240, %r79;
	@%p14 bra 	$L__BB3_12;
	bra.uni 	$L__BB3_11;

$L__BB3_12:
	mul.lo.s32 	%r244, %r77, 5570;
	add.s32 	%r245, %r83, %r84;
	mad.lo.s32 	%r246, %r245, -3801, %r244;
	and.b32  	%r247, %r12, 65535;
	and.b32  	%r248, %r28, 65535;
	add.s32 	%r249, %r248, %r247;
	and.b32  	%r250, %r60, 65535;
	add.s32 	%r251, %r249, %r250;
	add.s32 	%r252, %r251, %r250;
	mad.lo.s32 	%r253, %r252, 1016, %r246;
	shr.s32 	%r254, %r253, 2;
	add.s32 	%r255, %r80, %r81;
	mad.lo.s32 	%r288, %r255, 4309, %r254;
	mov.u32 	%r289, -213;
	bra.uni 	$L__BB3_13;

$L__BB3_11:
	add.s32 	%r242, %r80, %r81;
	mul.lo.s32 	%r288, %r242, 5077;
	mov.u32 	%r289, -981;

$L__BB3_13:
	shr.u32 	%r256, %r83, 1;
	sub.s32 	%r257, %r256, %r80;
	shr.u32 	%r258, %r84, 1;
	sub.s32 	%r259, %r258, %r81;
	min.s32 	%r260, %r257, %r259;
	sub.s32 	%r261, %r290, %r81;
	sub.s32 	%r262, %r290, %r80;
	max.s32 	%r263, %r261, %r262;
	max.s32 	%r264, %r257, %r259;
	min.s32 	%r265, %r261, %r262;
	min.s32 	%r266, %r265, %r264;
	max.s32 	%r267, %r263, %r260;
	neg.s32 	%r268, %r267;
	max.s32 	%r269, %r82, %r266;
	max.s32 	%r270, %r269, %r268;
	and.b32  	%r271, %r4, 65535;
	and.b32  	%r272, %r10, 65535;
	add.s32 	%r273, %r272, %r271;
	mad.lo.s32 	%r274, %r289, %r273, %r288;
	shr.s32 	%r275, %r274, 13;
	add.s32 	%r276, %r270, %r290;
	setp.gt.s32 	%p15, %r275, %r276;
	sub.s32 	%r277, %r290, %r270;
	max.s32 	%r278, %r275, %r277;
	selp.b32 	%r279, %r276, %r278, %p15;
	setp.lt.s32 	%p16, %r279, 0;
	min.s32 	%r280, %r279, %r95;
	selp.b32 	%r290, 0, %r280, %p16;

$L__BB3_14:
	cvt.u16.u32 	%rs3, %r290;
	cvt.u16.u32 	%rs4, %r287;
	st.global.v2.u16 	[%rd1], {%rs4, %rs3};

$L__BB3_16:
	ret;

}

